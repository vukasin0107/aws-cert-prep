{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to MkDocs","text":"<p>For full documentation visit mkdocs.org.</p>"},{"location":"#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre>"},{"location":"01.%20EC2/","title":"01. EC2","text":""},{"location":"01.%20EC2/#ec2-instances","title":"EC2 Instances","text":"<ol> <li>EC2 Instances Purchasing Options</li> <li>On-Demand Instances \u2013 short workload, predictable pricing, pay by second</li> <li>Reserved (1 &amp; 3 years)</li> <li>Reserved Instances \u2013 long workloads</li> <li>Convertible Reserved Instances \u2013 long workloads with flexible instances</li> <li>Savings Plans (1 &amp; 3 years) \u2013commitment to an amount of usage, long workload</li> <li>Spot Instances \u2013 short workloads, cheap, can lose instances (less reliable)</li> <li>Dedicated Hosts \u2013 book an entire physical server, control instance placement</li> <li>Dedicated Instances \u2013 no other customers will share your hardware</li> <li> <p>Capacity Reservations \u2013 reserve capacity in a specific AZ for any duration</p> </li> <li> <p>EC2 On Demand</p> </li> <li>Pay for what you use:</li> <li>Linux or Windows - billing per second, after the first minute</li> <li>All other operating systems - billing per hour</li> <li>Has the highest cost but no upfront payment</li> <li>No long-term commitment</li> <li> <p>Recommended for short-term and un-interrupted workloads, where you can't predict how the application will behave</p> </li> <li> <p>EC2 Reserved Instances</p> </li> <li>Up to 72% discount compared to On-demand</li> <li>You reserve a specific instance attributes (Instance Type, Region, Tenancy, OS)</li> <li>Reservation Period \u2013 1 year (+discount) or 3 years (+++discount)</li> <li>Payment Options \u2013 No Upfront (+), Partial Upfront (++), All Upfront (+++)</li> <li>Reserved Instance\u2019s Scope \u2013 Regional or Zonal (reserve capacity in an AZ)</li> <li>Recommended for steady-state usage applications (think database)</li> <li> <p>You can buy and sell in the Reserved Instance Marketplace</p> </li> <li> <p>Convertible Reserved Instance</p> <ul> <li>Can change the EC2 instance type, instance family, OS, scope and tenancy</li> <li>Up to 66% discount</li> </ul> </li> <li> <p>EC2 Savings Plans</p> </li> <li>Get a discount based on long-term usage (up to 72% - same as RIs)</li> <li>Commit to a certain type of usage ($10/hour for 1 or 3 years)</li> <li> <p>Usage beyond EC2 Savings Plans is billed at the On-Demand price</p> </li> <li> <p>Locked to a specific instance family &amp; AWS region (e.g., M5 in us-east-1)</p> </li> <li> <p>Flexible across:</p> <ul> <li>Instance Size (e.g., m5.xlarge, m5.2xlarge)</li> <li>OS (e.g., Linux, Windows)</li> <li>Tenancy (Host, Dedicated, Default)</li> </ul> </li> <li> <p>EC2 Spot Instances Can get a discount of up to 90% compared to On-demand</p> </li> <li>Instances that you can \u201close\u201d at any point of time if your max price is less than the current spot price</li> <li> <p>The MOST cost-efficient instances in AWS</p> </li> <li> <p>Useful for workloads that are resilient to failure:</p> <ul> <li>Batch jobs</li> <li>Data analysis</li> <li>Image processing</li> <li>Any distributed workloads</li> <li>Workloads with a flexible start and end time</li> </ul> </li> <li> <p>Not suitable for critical jobs or databases</p> </li> <li> <p>EC2 Dedicated Hosts A physical server with EC2 instance capacity fully dedicated to your use</p> </li> <li>Allows you address compliance requirements and use your existing server- bound software licenses (per-socket, per-core, pe\u2014VM software licenses)</li> <li>Purchasing Options:</li> <li>On-demand \u2013 pay per second for active Dedicated Host</li> <li>Reserved - 1 or 3 years (No Upfront, Partial Upfront, All Upfront)</li> <li>The most expensive option</li> <li>Useful for software that have complicated licensing model (BYOL \u2013 Bring Your Own License)</li> <li> <p>Or for companies that have strong regulatory or compliance needs</p> </li> <li> <p>EC2 Dedicated Instances </p> </li> <li>Instances run on hardware that\u2019s dedicated to you</li> <li>May share hardware with other instances in same account</li> <li> <p>No control over instance placement (can move hardware after Stop / Start)</p> </li> <li> <p>EC2 Spot Instance Requests</p> </li> <li>Can get a discount of up to 90% compared to On-demand</li> <li>Define max spot price and get the instance while current spot price &lt; max</li> <li>The hourly spot price varies based on offer and capacity</li> <li>If the current spot price &gt; your max price you can choose to stop or terminate your instance with a 2 minutes grace period.</li> <li>Other strategy: Spot Block</li> <li>\u201cblock\u201d spot instance during a specified time frame (1 to 6 hours) without interruptions</li> <li>In rare situations, the instance may be reclaimed</li> <li>Used for batch jobs, data analysis, or workloads that are resilient to failures.</li> <li>Not great for critical jobs or databases</li> <li> <p>Terminating Spot Instance:</p> <ul> <li>You can only cancel Spot Instance requests that are open, active, or disabled.</li> <li>Cancelling a Spot Request does not terminate instances</li> <li>You must first cancel a Spot Request, and then terminate the associated Spot Instances</li> </ul> </li> <li> <p>Spot Fleets</p> </li> <li>Spot Fleets = set of Spot Instances + (optional) On-Demand Instances</li> <li>The Spot Fleet will try to meet the target capacity with price constraints</li> <li>Define possible launch pools: instance type (m5.large), OS, Availability Zone</li> <li>Can have multiple launch pools, so that the fleet can choose</li> <li>Spot Fleet stops launching instances when reaching capacity or max cost</li> <li>Strategies to allocate Spot Instances:</li> <li>lowestPrice: from the pool with the lowest price (cost optimization, short workload)</li> <li>diversified: distributed across all pools (great for availability, long workloads)</li> <li>capacityOptimized: pool with the optimal capacity for the number of instances</li> <li>priceCapacityOptimized (recommended): pools with highest capacity available, then select the pool with the lowest price (best choice for most workloads)</li> <li> <p>Spot Fleets allow us to automatically request Spot Instances with the lowest price</p> </li> <li> <p>Placement Groups</p> </li> <li>Sometimes you want control over the EC2 Instance placement strategy</li> <li>That strategy can be defined using placement groups</li> <li> <p>When you create a placement group, you specify one of the following strategies for the group:</p> <ul> <li>Cluster \u2014 clusters instances into a low-latency group in a single Availability Zone(In the same rack<ul> <li>Pros: Great network (10 Gbps bandwidth between instances with Enhanced Networking enabled - recommended)</li> <li>Cons: If the rack fails, all instances fails at the same time</li> <li>Use case:</li> <li>Big Data job that needs to complete fast</li> <li>Application that needs extremely low latency and high network throughput)</li> </ul> </li> <li>Spread \u2014 spreads instances across underlying hardware (max 7 instances per group per AZ)<ul> <li>Pros:<ul> <li>Can span across Availability Zones (AZ)</li> <li>Reduced risk is simultaneous failure</li> <li>EC2 Instances are on different physical hardware</li> </ul> </li> <li>Cons:<ul> <li>Limited to 7 instances per AZ per placement group</li> </ul> </li> <li>Use case:<ul> <li>Application that needs to maximize high availability</li> <li>Critical Applications where each instance must be isolated from failure from each other</li> </ul> </li> </ul> </li> <li>Partition\u2014spreads instances across many different partitions (which rely on different sets of racks) within an AZ. Scales to 100s of EC2 instances per group (Hadoop, Cassandra, Kafka)<ul> <li>Up to 7 partitions per AZ</li> <li>Can span across multiple AZs in the same region</li> <li>Up to 100s of EC2 instances</li> <li>The instances in a partition do not share racks with the instances in the other partitions</li> <li>A partition failure can affect many EC2 but won\u2019t affect other partitions</li> <li>EC2 instances get access to the partition information as metadata</li> <li>Use cases: HDFS, HBase, Cassandra, Kafka</li> </ul> </li> </ul> </li> <li> <p>Elastic Network Interfaces (ENI)</p> </li> <li>https://aws.amazon.com/blogs/aws/new-elastic-network-interfaces-in-the-virtual-private-cloud/</li> <li>Logical component in a VPC that represents a virtual network card</li> <li>The ENI can have the following attributes:</li> <li>Primary private IPv4, one or more secondary IPv4</li> <li>One Elastic IP (IPv4) per private IPv4</li> <li>One Public IPv4</li> <li>One or more security groups</li> <li>A MAC address</li> <li>You can create ENI independently and attach them on the fly (move them) on EC2 instances for failover</li> <li> <p>Bound to a specific availability zone (AZ)</p> </li> <li> <p>EC2 Hibernate</p> </li> <li>Introducing EC2 Hibernate:</li> <li>The in-memory (RAM) state is preserved</li> <li>The instance boot is much faster! (the OS is not stopped / restarted)</li> <li>Under the hood: the RAM state is written to a file in the root EBS volume</li> <li>The root EBS volume must be encrypted</li> <li>Use cases:<ul> <li>Long-running processing</li> <li>Saving the RAM state</li> <li>Services that take time to initialize</li> </ul> </li> </ol> <p>Good to know: - Supported Instance Families \u2013 C3, C4, C5, I3, M3, M4, R3, R4, T2, T3, \u2026 - Instance RAM Size \u2013 must be less than 150 GB. - Instance Size \u2013 not supported for bare metal instances. - AMI \u2013 Amazon Linux 2, Linux AMI, Ubuntu, RHEL, CentOS &amp; Windows\u2026 - Root Volume \u2013 must be EBS, encrypted, not instance store, and large - Available for On-Demand, Reserved and Spot Instances - An instance can NOT be hibernated more than 60 days</p>"},{"location":"02.%20EC2%20Storage/","title":"02. EC2 Storage","text":"<ol> <li>EBS Volume  An EBS (Elastic Block Store) Volume is a network drive you can attach to your instances while they run.</li> <li>It allows your instances to persist data, even after their termination</li> <li>They can only be mounted to one instance at a time (at the CCP level)</li> <li>They are bound to a specific availability zone</li> <li>Analogy: Think of them as a \u201cnetwork USB stick\u201d</li> <li> <p>Free tier: 30 GB of free EBS storage of type General Purpose (SSD) or Magnetic per month</p> </li> <li> <p>It\u2019s a network drive (i.e. not a physical drive)</p> </li> <li>It uses the network to communicate the instance, which means there might be a bit of latency</li> <li>It can be detached from an EC2 instance and attached to another one quickly</li> <li>It\u2019s locked to an Availability Zone (AZ)</li> <li>An EBS Volume in us-east-1a cannot be attached to us-east-1b</li> <li>To move a volume across, you first need to snapshot it</li> <li>Have a provisioned capacity (size in GBs, and IOPS)</li> <li>You get billed for all the provisioned capacity</li> <li> <p>You can increase the capacity of the drive over time</p> </li> <li> <p>EBS \u2013 Delete on Termination attribute:     Controls the EBS behaviour when an EC2 instance terminates</p> <ul> <li>By default, the root EBS volume is deleted (attribute enabled)</li> <li>By default, any other attached EBS volume is not deleted (attribute disabled)</li> <li>This can be controlled by the AWS console / AWS CLI</li> <li>Use case: preserve root volume when instance is terminated</li> </ul> </li> <li> <p>EBS Snapshots</p> </li> <li>Make a backup (snapshot) of your EBS volume at a point in time</li> <li>Not necessary to detach volume to do snapshot, but recommended</li> <li> <p>Can copy snapshots across AZ or Region</p> </li> <li> <p>EBS Snapshots Features</p> </li> <li>EBS Snapshot Archive<ul> <li>Move a Snapshot to an \u201darchive tier\u201d that is 75% cheaper</li> <li>Takes within 24 to 72 hours for restoring the archive</li> </ul> </li> <li>Recycle Bin for EBS Snapshots<ul> <li>Setup rules to retain deleted snapshots so you can recover them after an accidental deletion</li> <li>Specify retention (from 1 day to 1 year)</li> </ul> </li> <li> <p>Fast Snapshot Restore (FSR)</p> <ul> <li>Force full initialization of snapshot to have no latency on the first use ($$$)</li> </ul> </li> <li> <p>AMI Overview AMI = Amazon Machine Image</p> </li> <li>AMI are a customization of an EC2 instance</li> <li>You add your own software, configuration, operating system, monitoring\u2026</li> <li>Faster boot / configuration time because all your software is pre-packaged</li> <li>AMI are built for a specific region (and can be copied across regions)</li> <li>You can launch EC2 instances from:</li> <li>A Public AMI: AWS provided</li> <li>Your own AMI: you make and maintain them yourself</li> <li> <p>An AWS Marketplace AMI: an AMI someone else made</p> <p>AMI Process (from an EC2 instance) - Start an EC2 instance and customize it - Stop the instance (for data integrity) - Build an AMI \u2013 this will also create EBS snapshots - Launch instances from other AMIs</p> </li> <li> <p>EC2 Instance Store</p> </li> <li>EBS volumes are network drives with good but \u201climited\u201d performance</li> <li>If you need a high-performance hardware disk, use EC2 Instance Store</li> <li>Better I/O performance</li> <li>EC2 Instance Store lose their storage if they\u2019re stopped (ephemeral)</li> <li>Good for buffer / cache / scratch data / temporary content</li> <li>Risk of data loss if hardware fails</li> <li> <p>Backups and Replication are your responsibility</p> </li> <li> <p>Local EC2 Instance Store </p> </li> <li> <p>EBS Volume Types</p> </li> <li>EBS Volumes come in 6 types<ul> <li>gp2 / gp3 (SSD): General purpose SSD volume that balances price and performance for a wide variety of workloads</li> <li>io1 / io2 Block Express (SSD): Highest-performance SSD volume for mission-critical low-latency or high-throughput workloads</li> </ul> </li> <li>st1 (HDD): Low cost HDD volume designed for frequently accessed, throughput- intensive workloads</li> <li> <p>sc1 (HDD): Lowest cost HDD volume designed for less frequently accessed workloads</p> </li> <li> <p>EBS Volumes are characterized in Size | Throughput | IOPS (I/O Ops Per Sec)</p> </li> <li>When in doubt always consult the AWS documentation \u2013 it\u2019s good!</li> <li> <p>Only gp2/gp3 and io1/io2 Block Express can be used as boot volumes</p> </li> <li> <p>EBS Volume Types Use cases General Purpose SSD</p> </li> <li>Cost effective storage, low-latency</li> <li>System boot volumes, Virtual desktops, Development and test environments</li> <li>1 GiB - 16 TiB</li> <li>gp3:<ul> <li>Baseline of 3,000 IOPS and throughput of 125 MiB/s</li> <li>Can increase IOPS up to 16,000 and throughput up to 1000 MiB/s independently</li> </ul> </li> <li> <p>gp2:</p> <ul> <li>Small gp2 volumes can burst IOPS to 3,000</li> <li>Size of the volume and IOPS are linked, max IOPS is 16,000</li> <li>3 IOPS per GB, means at 5,334 GB we are at the max IOPS</li> </ul> </li> <li> <p>EBS Volume Types Use cases Provisioned IOPS (PIOPS) SSD</p> </li> <li>Critical business applications with sustained IOPS performance</li> <li>Or applications that need more than 16,000 IOPS</li> <li>Great for databases workloads (sensitive to storage perf and consistency)</li> <li>io1 (4 GiB - 16 TiB):<ul> <li>Max PIOPS: 64,000 for Nitro EC2 instances &amp; 32,000 for other</li> <li>Can increase PIOPS independently from storage size</li> </ul> </li> <li>io2 Block Express (4 GiB \u2013 64 TiB):<ul> <li>Sub-millisecond latency</li> <li>Max PIOPS: 256,000 with an IOPS:GiB ratio of 1,000:1</li> </ul> </li> <li> <p>Supports EBS Multi-attach </p> </li> <li> <p>EBS Volume Types Use cases Hard Disk Drives (HDD) </p> </li> <li>Cannot be a boot volume </li> <li>125 GiB to 16 TiB </li> <li>Throughput Optimized HDD (st1) <ul> <li>Big Data, Data Warehouses, Log Processing </li> <li>Max throughput 500 MiB/s \u2013 max IOPS 500 </li> </ul> </li> <li>Cold HDD (sc1): <ul> <li>For data that is infrequently accessed </li> <li>Scenarios where lowest cost is important </li> <li>Max throughput 250 MiB/s \u2013 max IOPS 250</li> </ul> </li> </ol> <p>Don't need to get all details but some top level differences, like which one to use to get the most IOPS at the lowest cost for a specific use case</p> <ol> <li>EBS Multi-Attach \u2013 io1/io2 family</li> <li>Attach the same EBS volume to multiple EC2 instances in the same AZ</li> <li>Each instance has full read &amp; write permissions to the high-performance volume</li> <li>Use case:<ul> <li>Achieve higher application availability in clustered Linux applications (ex: Teradata)</li> <li>Applications must manage concurrent write operations</li> </ul> </li> <li>Up to 16 EC2 Instances at a time</li> <li> <p>Must use a file system that\u2019s cluster-aware (not XFS, EXT4, etc\u2026) </p> </li> <li> <p>EBS Encryption</p> </li> <li>When you create an encrypted EBS volume, you get the following:<ul> <li>Data at rest is encrypted inside the volume</li> <li>All the data in flight moving between the instance and the volume is encrypted</li> <li>All snapshots are encrypted</li> <li>All volumes created from the snapshot</li> </ul> </li> <li>Encryption and decryption are handled transparently (you have nothing to do)</li> <li>Encryption has a minimal impact on latency</li> <li>EBS Encryption leverages keys from KMS (AES-256)</li> <li>Copying an unencrypted snapshot allows encryption</li> <li> <p>Snapshots of encrypted volumes are encrypted</p> </li> <li> <p>Encryption: encrypt an unencrypted EBS volume</p> </li> <li>Create an EBS snapshot of the volume</li> <li>Encrypt the EBS snapshot ( using copy )</li> <li>Create new ebs volume from the snapshot ( the volume will also be encrypted )</li> <li> <p>Now you can attach the encrypted volume to the original instance</p> </li> <li> <p>Amazon EFS \u2013 Elastic File System</p> </li> <li>Managed NFS (network file system) that can be mounted on many EC2</li> <li>EFS works with EC2 instances in multi-AZ</li> <li> <p>Highly available, scalable, expensive (3x gp2), pay per use</p> </li> <li> <p>Amazon EFS \u2013 Elastic File System</p> </li> <li>Use cases: content management, web serving, data sharing, Wordpress</li> <li>Uses NFSv4.1 protocol</li> <li>Uses security group to control access to EFS</li> <li>Compatible with Linux based AMI (not Windows)</li> <li>Encryption at rest using KMS</li> <li>POSIX file system (~Linux) that has a standard file API</li> <li> <p>File system scales automatically, pay-per-use, no capacity planning!</p> </li> <li> <p>EFS \u2013 Performance &amp; Storage Classes</p> </li> <li>EFS Scale<ul> <li>1000s of concurrent NFS clients, 10 GB+ /s throughput</li> <li>Grow to Petabyte-scale network file system, automatically</li> </ul> </li> <li>Performance Mode (set at EFS creation time)<ul> <li>General Purpose (default) \u2013 latency-sensitive use cases (web server, CMS, etc\u2026)</li> <li>Max I/O \u2013 higher latency, throughput, highly parallel (big data, media processing)</li> </ul> </li> <li> <p>Throughput Mode</p> <ul> <li>Bursting \u2013 1 TB = 50MiB/s + burst of up to 100MiB/s</li> <li>Provisioned \u2013 set your throughput regardless of storage size, ex: 1 GiB/s for 1 TB storage</li> <li>Elastic \u2013 automatically scales throughput up or down based on your workloads<ul> <li>Up to 3GiB/s for reads and 1GiB/s for writes</li> <li>Used for unpredictable workloads</li> </ul> </li> </ul> </li> <li> <p>EFS \u2013 Storage Classes</p> </li> <li>Storage Tiers (lifecycle management feature \u2013 move file after N days) <ul> <li>Standard: for frequently accessed files </li> <li>Infrequent access (EFS-IA): cost to retrieve files, lower price to store. Enable EFS-IA with a LifecyclePolicy</li> </ul> </li> <li>Availability and durability <ul> <li>Standard: Multi-AZ, great for prod </li> <li>One Zone: One AZ, great for dev, backup enabled by default, compatible with IA (EFS One Zone-IA)</li> </ul> </li> <li> <p>Over 90% in cost savings</p> </li> <li> <p>EBS vs EFS \u2013 Elastic Block Storage</p> </li> <li> <p>EBS volumes\u2026</p> <ul> <li>one instance (except multi-attach io1/io2)</li> <li>are locked at the Availability Zone (AZ) level</li> <li>gp2: IO increases if the disk size increases</li> <li>gp3 &amp; io1: can increase IO independently</li> </ul> </li> <li>To migrate an EBS volume across AZ<ul> <li>Take a snapshot</li> <li>Restore the snapshot to another AZ</li> <li>EBS backups use IO and you shouldn\u2019t run them while your application is handling a lot of traffic</li> </ul> </li> <li> <p>Root EBS Volumes of instances get terminated by default if the EC2 instanc</p> </li> <li> <p>EBS vs EFS \u2013 Elastic File System</p> </li> <li>Mounting 100s of instances across AZ</li> <li>EFS share website files (WordPress)</li> <li>Only for Linux Instances (POSIX)</li> <li>EFS has a higher price point than EBS</li> <li>Can leverage EFS-IA for cost savings</li> <li>Remember: EFS vs EBS vs Instance Store</li> </ol>"},{"location":"03.%20ELB%20and%20ASG/","title":"03. ELB and ASG","text":""},{"location":"03.%20ELB%20and%20ASG/#elastic-load-balancer","title":"ELASTIC LOAD BALANCER","text":"<ol> <li>Why use a load balancer? </li> <li>Spread load across multiple downstream instances </li> <li>Expose a single point of access (DNS) to your application </li> <li>Seamlessly handle failures of downstream instances </li> <li>Do regular health checks to your instances </li> <li>Provide SSL termination (HTTPS) for your websites </li> <li>Enforce stickiness with cookies </li> <li>High availability across zones </li> <li> <p>Separate public traffic from private traffic</p> </li> <li> <p>Why use an Elastic Load Balancer?</p> </li> <li>An Elastic Load Balancer is a managed load balancer</li> <li>AWS guarantees that it will be working</li> <li>AWS takes care of upgrades, maintenance, high availability</li> <li>AWS provides only a few configuration knobs</li> <li>It costs less to setup your own load balancer but it will be a lot more effort on your end</li> <li>It is integrated with many AWS offerings / services</li> <li>EC2, EC2 Auto Scaling Groups, Amazon ECS</li> <li>AWS Certificate Manager (ACM), CloudWatch</li> <li> <p>Route 53, AWS WAF, AWS Global Accelerator</p> </li> <li> <p>Health Checks</p> </li> <li>Health Checks are crucial for Load Balancers</li> <li>They enable the load balancer to know if instances it forwards traffic to are available to reply to requests</li> <li>The health check is done on a port and a route (/health is common)</li> <li> <p>If the response is not 200 (OK), then the instance is unhealthy</p> </li> <li> <p>Types of load balancer on AWS</p> </li> <li>AWS has 4 kinds of managed Load Balancers</li> <li>Classic Load Balancer (v1 - old generation) \u2013 2009 \u2013 CLB<ul> <li>HTTP, HTTPS, TCP, SSL (secure TCP)</li> <li>Supports TCP (Layer 4), HTTP &amp; HTTPS (Layer 7)</li> <li>Health checks are TCP or HTTP based</li> <li>Fixed hostname XXX.region.elb.amazonaws.com</li> </ul> </li> <li>Application Load Balancer (v2 - new generation) \u2013 2016 \u2013 ALB<ul> <li>HTTP, HTTPS, WebSocket</li> <li>Load balancing to multiple HTTP applications across machines (target groups)</li> <li>Load balancing to multiple applications on the same machine (ex: containers)</li> <li>Support for HTTP/2 and WebSocket</li> <li>Support redirects (from HTTP to HTTPS for example)</li> <li>Routing tables to different target groups:<ul> <li>Routing based on path in URL (example.com/users &amp; example.com/posts)</li> <li>Routing based on hostname in URL (one.example.com &amp; other.example.com)</li> <li>Routing based on Query String, Headers (example.com/users?id=123&amp;order=false)</li> </ul> </li> <li>ALB are a great fit for micro services &amp; container-based application (example: Docker &amp; Amazon ECS)</li> <li>Has a port mapping feature to redirect to a dynamic port in ECS</li> <li>In comparison, we\u2019d need multiple Classic Load Balancer per application</li> <li>Target Groups<ul> <li>EC2 instances (can be managed by an Auto Scaling Group) \u2013 HTTP</li> <li>ECS tasks (managed by ECS itself) \u2013 HTTP</li> <li>Lambda functions \u2013 HTTP request is translated into a JSON event</li> <li>IP Addresses \u2013 must be private IPs</li> <li>ALB can route to multiple target groups</li> <li>Health checks are at the target group leve</li> </ul> </li> </ul> </li> <li>Network Load Balancer (v2 - new generation) \u2013 2017 \u2013 NLB<ul> <li>TCP, TLS (secure TCP), UDP</li> <li>Network load balancers (Layer 4) allow to:<ul> <li>Forward TCP &amp; UDP traffic to your instances</li> <li>Handle millions of request per seconds</li> <li>Less latency ~100 ms (vs 400 ms for ALB)</li> </ul> </li> <li>NLB has one static IP per AZ, and supports assigning Elastic IP (helpful for whitelisting specific IP)</li> <li>NLB are used for extreme performance, TCP or UDP traffic</li> <li>Not included in the AWS free tier</li> <li>Network Load Balancer \u2013Target Groups<ul> <li>EC2 instances</li> <li>IP Addresses \u2013 must be private IPs</li> <li>Application Load Balancer (thanks to NLB you get fixed Ips and thanks tou ALB you get all rules and features)</li> <li>Health Checks support the TCP, HTTP and HTTPS Protocols</li> </ul> </li> </ul> </li> <li>Gateway Load Balancer \u2013 2020 \u2013 GWLB<ul> <li>Operates at layer 3 (Network layer) \u2013 IP Protocol</li> <li>https://synechron.udemy.com/course/aws-certified-solutions-architect-associate-saa-c03/learn/lecture/28874688#overview</li> <li>Deploy, scale, and manage a fleet of 3<sup>rd</sup> party network virtual appliances in AWS</li> <li>Example: Firewalls, Intrusion Detection and Prevention Systems, Deep Packet Inspection Systems, payload manipulation, \u2026</li> <li>Operates at Layer 3 (Network Layer) \u2013 IP Packets</li> <li>Combines the following functions:</li> <li>Transparent Network Gateway \u2013 single entry/exit for all traffic</li> <li>Load Balancer \u2013 distributes traffic to your virtual appliances</li> <li>Uses the GENEVE protocol on port 6081</li> <li>Gateway Load Balancer \u2013Target Groups<ul> <li>EC2 instances</li> <li>IP Addresses \u2013 must be private IPs</li> </ul> </li> </ul> </li> <li>Overall, it is recommended to use the newer generation load balancers as they provide more features</li> <li> <p>Some load balancers can be setup as internal (private) or external (public) ELBs</p> </li> <li> <p>Sticky Sessions (Session Affinity)</p> </li> <li>It is possible to implement stickiness so that the same client is always redirected to the same instance behind a load balancer</li> <li>This works for Classic Load Balancer, Application Load Balancer, and Network Load Balancer</li> <li>For both CLB &amp; ALB, the \u201ccookie\u201d used for stickiness has an expiration date you control</li> <li>Use case: make sure the user doesn\u2019t lose his session data</li> <li> <p>Enabling stickiness may bring imbalance to the load over the backend EC2 instances</p> </li> <li> <p>Sticky Sessions \u2013 Cookie Names This is configured inside target group(attributes)</p> </li> <li> <p>Application-based Cookies</p> <ul> <li>Custom cookie<ul> <li>Generated by the target</li> <li>Can include any custom attributes required by the application</li> <li>Cookie name must be specified individually for each target group</li> <li>Don\u2019t use AWSALB, AWSALBAPP, or AWSALBTG (reserved for use by the ELB)</li> </ul> </li> <li>Application cookie<ul> <li>Generated by the load balancer</li> <li>Cookie name is AWSALBAPP</li> </ul> </li> <li>Duration-based Cookies<ul> <li>Cookie generated by the load balancer</li> <li>Cookie name is AWSALB for ALB, AWSELB for CLB</li> </ul> </li> </ul> </li> <li> <p>Cross-Zone Load Balancing</p> <ul> <li>With Cross Zone Load Balancing: each load balancer instance distributes evenly across all registered instances in all AZ</li> <li>Without Cross Zone Load Balancing: Requests are distributed in the instances of the node of the Elastic Load Balancer </li> </ul> <p>It is configured on the ELB itself, attributes section</p> <ul> <li>Application Load Balancer  <ul> <li>Enabled by default (can be disabled at the Target Group level)  </li> <li>No charges for inter AZ data  </li> </ul> </li> <li>Network Load Balancer &amp; Gateway Load Balancer  <ul> <li>Disabled by default  </li> <li>You pay charges ($) for inter AZ data if enabled  </li> </ul> </li> <li>Classic Load Balancer  <ul> <li>Disabled by default  </li> <li>No charges for inter AZ data if enabled  </li> </ul> </li> </ul> </li> <li> <p>Load Balancer - SSL Certificates</p> </li> <li> <p>The load balancer uses an X.509 certificate (SSL/TLS server certificate)</p> </li> <li>You can manage certificates using ACM (AWS Certificate Manager)</li> <li>You can create upload your own certificates alternatively</li> <li> <p>HTTPS listener:</p> <ul> <li>You must specify a default certificate</li> <li>You can add an optional list of certs to support multiple domains</li> <li>Clients can use SNI (Server Name Indication) to specify the hostname they reach</li> <li>Ability to specify a security policy to support older versions of SSL / TLS (legacy clients)</li> </ul> </li> <li> <p>SSL \u2013 Server Name Indication (SNI)</p> </li> <li></li> <li>SNI solves the problem of loading multiple SSL certificates onto one web server (to serve multiple websites)</li> <li>It\u2019s a \u201cnewer\u201d protocol, and requires the client to indicate the hostname of the target server in the initial SSL handshake</li> <li>The server will then find the correct certificate, or return the default one</li> </ol> <p>Note: - Only works for ALB &amp; NLB (newer generation), CloudFront - Does not work for CLB (older gen)</p> <ol> <li> <p>Elastic Load Balancers \u2013 SSL Certificates</p> </li> <li> <p>Classic Load Balancer (v1)</p> <ul> <li>Support only one SSL certificate</li> <li>Must use multiple CLB for multiple hostname with multiple SSL certificates</li> </ul> </li> <li>Application Load Balancer (v2)<ul> <li>Supports multiple listeners with multiple SSL certificates</li> <li>Uses Server Name Indication (SNI) to make it work</li> </ul> </li> <li> <p>Network Load Balancer (v2)</p> <ul> <li>Supports multiple listeners with multiple SSL certificates</li> <li>Uses Server Name Indication (SNI) to make it work</li> </ul> </li> <li> <p>Connection Draining</p> </li> <li>Feature naming<ul> <li>Connection Draining \u2013 for CLB</li> <li>Deregistration Delay \u2013 for ALB &amp; NLB</li> </ul> </li> <li>Time to complete \u201cin-flight requests\u201d while the instance is de-registering or unhealthy</li> <li>Stops sending new requests to the EC2 instance which is de-registering</li> <li>Between 1 to 3600 seconds (default: 300 seconds)</li> <li>Can be disabled (set value to 0)</li> <li>Set to a low value if your requests are short</li> </ol>"},{"location":"03.%20ELB%20and%20ASG/#auto-scaling-group","title":"AUTO SCALING GROUP","text":"<ol> <li>What\u2019s an Auto Scaling Group?</li> <li>In real-life, the load on your websites and application can change</li> <li> <p>In the cloud, you can create and get rid of servers very quickly</p> </li> <li> <p>The goal of an Auto Scaling Group (ASG) is to:</p> <ul> <li>Scale out (add EC2 instances) to match an increased load</li> <li>Scale in (remove EC2 instances) to match a decreased load</li> <li>Ensure we have a minimum and a maximum number of EC2 instances running</li> <li>Automatically register new instances to a load balancer</li> <li>Re-create an EC2 instance in case a previous one is terminated (ex: if unhealthy)</li> </ul> </li> <li> <p>ASG are free (you only pay for the underlying EC2 instances) </p> </li> <li> <p>Auto Scaling Group Attributes</p> </li> <li>A Launch Template (older \u201cLaunch Configurations\u201d are deprecated)<ul> <li>AMI + Instance Type</li> <li>EC2 User Data</li> <li>EBS Volumes</li> <li>Security Groups</li> <li>SSH Key Pair</li> <li>IAM Roles for your EC2 Instances</li> <li>Network + Subnets Information</li> <li>Load Balancer Information</li> <li>Min Size / Max Size / Initial Capacity</li> </ul> </li> <li> <p>Scaling Policies</p> </li> <li> <p>Auto Scaling - CloudWatch Alarms &amp; Scaling</p> </li> <li>It is possible to scale an ASG based on CloudWatch alarms</li> <li>An alarm monitors a metric (such as Average CPU, or a custom metric)</li> <li>Metrics such as Average CPU are computed for the overall ASG instances</li> <li> <p>Based on the alarm:</p> <ul> <li>We can create scale-out policies (increase the number of instances)</li> <li>We can create scale-in policies (decrease the number of instances)</li> </ul> </li> <li> <p>Auto Scaling Groups \u2013 Scaling Policies</p> </li> <li> <p>Dynamic Scaling</p> <ul> <li>Target Tracking Scaling   <ul> <li>Simple to set-up</li> <li>Example: I want the average ASG CPU to stay at around 40%</li> </ul> </li> <li>Simple / Step Scaling<ul> <li>When a CloudWatch alarm is triggered (example CPU &gt; 70%), then add 2 units</li> <li>When a CloudWatch alarm is triggered (example CPU &lt; 30%), then remove 1</li> </ul> </li> <li>Scheduled Scaling<ul> <li>Anticipate a scaling based on known usage patterns</li> <li>Example: increase the min capacity to 10 at 5 pm on Fridays</li> </ul> </li> <li>Predictive scaling: continuously forecast load and schedule scaling ahead</li> </ul> </li> <li> <p>Good metrics to scale on</p> </li> <li>CPUUtilization: Average CPU utilization across your instances</li> <li>RequestCountPerTarget: to make sure the number of requests per EC2 instances is stable</li> <li>Average Network In / Out (if you\u2019re application is network bound)</li> <li> <p>Any custom metric (that you push using CloudWatch)</p> </li> <li> <p>Auto Scaling Groups - Scaling Cooldowns</p> </li> <li>After a scaling activity happens, you are in the cooldown period (default 300 seconds)</li> <li>During the cooldown period, the ASG will not launch or terminate additional instances (to allow for metrics to stabilize)</li> <li>Advice: Use a ready-to-use AMI to reduce configuration time in order to be serving request fasters and reduce the cooldown period</li> </ol>"},{"location":"04.%20RDS-Aurora-ElastiCache/","title":"04. RDS Aurora ElastiCache","text":"<ol> <li>Amazon RDS Overview </li> <li>Tool for connecting to DB sqlectron -&gt; https://github.com/sqlectron/sqlectron-gui/releases/tag/v1.38 .0</li> <li>RDS stands for Relational Database Service</li> <li>It\u2019s a managed DB service for DB use SQL as a query language.</li> <li> <p>It allows you to create databases in the cloud that are managed by AWS</p> <ul> <li>Postgres</li> <li>MySQL</li> <li>MariaDB</li> <li>Oracle</li> <li>Microsoft SQL Server</li> <li>Aurora (AWS Proprietary database)</li> </ul> </li> <li> <p>Advantage over using RDS versus deploying DB on EC2</p> </li> <li>RDS is a managed service:<ul> <li>Automated provisioning, OS patching</li> <li>Continuous backups and restore to specific timestamp (Point in Time Restore)!</li> <li>Monitoring dashboards</li> <li>Read replicas for improved read performance</li> <li>Multi AZ setup for DR (Disaster Recovery)</li> <li>Maintenance windows for upgrades</li> <li>Scaling capability (vertical and horizontal)</li> <li>Storage backed by EBS (gp2 or io1)</li> </ul> </li> <li> <p>BUT you can\u2019t SSH into your instances</p> </li> <li> <p>RDS \u2013 Storage Auto Scaling</p> </li> <li>Helps you increase storage on your RDS DB instance dynamically</li> <li>When RDS detects you are running out of free database storage, it scales automatically</li> <li>Avoid manually scaling your database storage </li> <li>You have to set Maximum Storage Threshold (maximum limit for DB storage) </li> <li>Automatically modify storage if:<ul> <li>Free storage is less than 10% of allocated storage </li> <li>Low-storage lasts at least 5 minutes </li> <li>6 hours have passed since last modification </li> </ul> </li> <li>Useful for applications with unpredictable workloads </li> <li> <p>Supports all RDS database engines (MariaDB, MySQL, PostgreSQL, SQL Server, Oracle)</p> </li> <li> <p>RDS Read Replicas for read scalability</p> </li> <li>Up to 15 Read Replicas</li> <li>Within AZ, Cross AZ or Cross Region</li> <li>Replication is ASYNC, so reads are eventually consistent</li> <li>Replicas can be promoted to their own DB</li> <li> <p>Applications must update the connection string to leverage read replicas </p> </li> <li> <p>RDS Read Replicas \u2013 Use Cases</p> </li> <li>You have a production database that is taking on normal load</li> <li>You want to run a reporting application to run some analytics</li> <li>You create a Read Replica to run the new workload there</li> <li>The production application is unaffected</li> <li> <p>Read replicas are used for SELECT (=read) only kind of statements (not INSERT, UPDATE, DELETE)</p> </li> <li> <p>RDS Read Replicas \u2013 Network Cost </p> </li> <li>In AWS there\u2019s a network cost when data goes from one AZ to another</li> <li> <p>For RDS Read Replicas within the same region, you don\u2019t pay that fee </p> </li> <li> <p>RDS Multi AZ (Disaster Recovery)</p> </li> <li>SYNC replication</li> <li>One DNS name \u2013 automatic app failover to standby</li> <li>Increase availability</li> <li>Failover in case of loss of AZ, loss of network, instance or storage failure</li> <li>No manual intervention in apps</li> <li>Not used for scaling</li> <li> <p>Note:The Read Replicas be setup as Multi AZ for Disaster Recovery (DR)</p> </li> <li> <p>RDS \u2013 From Single-AZ to Multi-AZ</p> </li> <li> <p>Zero downtime operation (no need to stop the DB)</p> </li> <li>Just click on \u201cmodify\u201d for the database</li> <li> <p>The following happens internally:</p> <ul> <li>A snapshot is taken</li> <li>A new DB is restored from the snapshot in a new AZ</li> <li>Synchronization is established between the two databases</li> </ul> </li> <li> <p>RDS Custom</p> </li> <li>Managed Oracle and Microsoft SQL Server Database with OS and database customization</li> <li>RDS: Automates setup, operation, and scaling of database in AWS</li> <li>Custom: access to the underlying database and OS so you can<ul> <li>Configure settings</li> <li>Install patches</li> <li>Enable native features</li> <li>Access the underlying EC2 Instance using SSH or SSM Session Manager</li> </ul> </li> <li>De-activate Automation Mode to perform your customization, better to take a DB snapshot before</li> <li> <p>RDS vs. RDS Custom</p> <ul> <li>RDS: entire database and the OS to be managed by AWS</li> <li>RDS Custom: full admin access to the underlying OS and the database</li> </ul> </li> <li> <p>Amazon Aurora</p> </li> <li>Aurora is a proprietary technology from AWS (not open sourced)</li> <li>Postgres and MySQL are both supported as Aurora DB (that means your drivers will work as if Aurora was a Postgres or MySQL database)</li> <li>Aurora is \u201cAWS cloud optimized\u201d and claims 5x performance improvement over MySQL on RDS, over 3x the performance of Postgres on RDS</li> <li>Aurora storage automatically grows in increments of 10GB, up to 128 TB.</li> <li>Aurora can have up to 15 replicas and the replication process is faster than MySQL (sub 10 ms replica lag)</li> <li>Failover in Aurora is instantaneous. It\u2019s HA (High Availability) native.</li> <li> <p>Aurora costs more than RDS (20% more) \u2013 but is more efficient</p> </li> <li> <p>Aurora High Availability and Read Scaling</p> </li> <li>6 copies of your data across 3 AZ:<ul> <li>4 copies out of 6 needed for writes</li> <li>3 copies out of 6 need for reads</li> <li>Self healing with peer-to-peer replication</li> <li>Storage is striped across 100s of volumes</li> </ul> </li> <li>One Aurora Instance takes writes (master)</li> <li>Automated failover for master in less than 30 seconds</li> <li>Master + up to 15 Aurora Read Replicas serve reads</li> <li> <p>Support for Cross Region Replication </p> </li> <li> <p>Aurora DB Cluster </p> </li> <li> <p>Features of Aurora</p> </li> <li>Automatic fail-over</li> <li>Backup and Recovery</li> <li>Isolation and security</li> <li>Industry compliance</li> <li>Push-button scaling</li> <li>Automated Patching with Zero Downtime</li> <li>Advanced Monitoring</li> <li>Routine Maintenance</li> <li> <p>Backtrack: restore data at any point of time without using backups</p> </li> <li> <p>Aurora Replicas - Auto Scaling </p> </li> <li> <p>Aurora \u2013 Custom Endpoints</p> </li> <li>Define a subset of Aurora Instances as a Custom Endpoint</li> <li>Example: Run analytical queries on specific replicas</li> <li> <p>The Reader Endpoint is generally not used after defining Custom Endpoints </p> </li> <li> <p>Aurora Serverless</p> </li> <li>Automated database instantiation and auto - scaling based on actual usage</li> <li>Good for infrequent, intermittent or unpredictable workloads</li> <li>No capacity planning needed</li> <li> <p>Pay per second, can be more cost-effective</p> </li> <li> <p>Global Aurora</p> </li> <li>Aurora Cross Region Read Replicas:</li> <li>Useful for disaster recovery</li> <li>Simple to put in place</li> <li>Aurora Global Database (recommended):</li> <li>1 Primary Region (read / write)</li> <li>Up to 5 secondary (read-only) regions, replication lag is less than 1 second</li> <li>Up to 16 Read Replicas per secondary region</li> <li>Helps for decreasing latency</li> <li>Promoting another region (for disaster recovery) has an RTO of &lt; 1 minute</li> <li> <p>Typical cross-region replication takes less than 1 second</p> </li> <li> <p>Aurora Machine Learning</p> </li> <li>Enables you to add ML-based predictions to your applications via SQL</li> <li>Simple, optimized, and secure integration between Aurora and AWS ML services</li> <li>Supported services</li> <li>Amazon SageMaker (use with any ML model)</li> <li>Amazon Comprehend (for sentiment analysis)</li> <li>You don\u2019t need to have ML experience</li> <li> <p>Use cases: fraud detection, ads targeting, sentiment analysis, product recommendations</p> </li> <li> <p>RDS Backups Automated backups:</p> <ul> <li>Daily full backup of the database (during the backup window)</li> <li>Transaction logs are backed-up by RDS every 5 minutes</li> <li>=&gt; ability to restore to any point in time (from oldest backup to 5 minutes ago)</li> <li>1 to 35 days of retention, set 0 to disable automated backups</li> </ul> </li> <li> <p>Manual DB Snapshots</p> <ul> <li>Manually triggered by the user</li> <li>Retention of backup for as long as you want</li> </ul> </li> <li> <p>Trick: in a stopped RDS database, you will still pay for storage. If you plan on stopping it for a long time, you should snapshot &amp; restore instead</p> </li> <li> <p>Aurora Backups</p> </li> <li>Automated backups <ul> <li>1 to 35 days (cannot be disabled) </li> <li>point-in-time recovery in that timeframe </li> </ul> </li> <li> <p>Manual DB Snapshots</p> <ul> <li>Manually triggered by the user </li> <li>Retention of backup for as long as you want</li> </ul> </li> <li> <p>RDS &amp; Aurora Restore options</p> </li> <li>Restoring a RDS / Aurora backup or a snapshot creates a new database</li> <li>Restoring MySQL RDS database from S3<ul> <li>Create a backup of your on-premises database</li> <li>Store it on Amazon S3 (object storage)</li> <li>Restore the backup file onto a new RDS instance running MySQL</li> </ul> </li> <li> <p>Restoring MySQL Aurora cluster from S3</p> <ul> <li>Create a backup of your on-premises database using Percona XtraBackup</li> <li>Store the backup file on Amazon S3</li> <li>Restore the backup file onto a new Aurora cluster running MySQL</li> </ul> </li> <li> <p>Aurora Database Cloning</p> </li> <li>Create a new Aurora DB Cluster from an existing one</li> <li>Faster than snapshot &amp; restore</li> <li>Uses copy-on-write protocol<ul> <li>Initially, the new DB cluster uses the same data volume as the original DB cluster (fast and efficient \u2013 no copying is needed)</li> <li>When updates are made to the new DB clusterdata, then additional storage is allocated and data is copied to be separated</li> </ul> </li> <li>Very fast &amp; cost-effective</li> <li>Useful to create a \u201cstaging\u201d database from a \u201cproduction\u201d database without impacting the production database</li> </ol>"},{"location":"05.%20Databases/","title":"05. Databases","text":"<ol> <li>Choosing the Right Database</li> <li>We have a lot of managed databases on AWS to choose from</li> <li> <p>Questions to choose the right database based on your architecture:</p> <ul> <li>Read-heavy, write-heavy, or balanced workload? Throughput needs? Will it change, does it need to scale or fluctuate during the day?</li> <li>How much data to store and for how long? Will it grow? Average object size? How are they accessed?</li> <li>Data durability? Source of truth for the data ?</li> <li>Latency requirements? Concurrent users?</li> <li>Data model? How will you query the data? Joins? Structured? Semi-Structured?</li> <li>Strong schema? More flexibility? Reporting? Search? RDBMS / NoSQL?</li> <li>License costs? Switch to Cloud Native DB such as Aurora?</li> </ul> </li> <li> <p>Database Types</p> </li> <li>RDBMS (= SQL / OLTP): RDS, Aurora \u2013 great for joins</li> <li>NoSQL database \u2013 no joins, no SQL : DynamoDB (~JSON), ElastiCache (key / value pairs), Neptune (graphs), DocumentDB (for MongoDB), Keyspaces (for Apache Cassandra)</li> <li>Object Store: S3 (for big objects) / Glacier (for backups / archives)</li> <li>Data Warehouse (= SQL Analytics / BI): Redshift (OLAP), Athena, EMR</li> <li>Search: OpenSearch (JSON) \u2013 free text, unstructured searches</li> <li>Graphs: Amazon Neptune \u2013 displays relationships between data</li> <li>Ledger: Amazon Quantum Ledger Database</li> <li>Time series: Amazon Timestream</li> <li>Note: some databases are being discussed in the Data &amp; Analytics section</li> </ol>"},{"location":"10.%20AWS%20Organizations/","title":"10. AWS Organizations","text":""}]}