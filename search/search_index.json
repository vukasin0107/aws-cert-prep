{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to MkDocs For full documentation visit mkdocs.org . Commands mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit. Project layout mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Welcome to MkDocs"},{"location":"#welcome-to-mkdocs","text":"For full documentation visit mkdocs.org .","title":"Welcome to MkDocs"},{"location":"#commands","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit.","title":"Commands"},{"location":"#project-layout","text":"mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Project layout"},{"location":"01.%20EC2/","text":"EC2 Instances EC2 Instances Purchasing Options On-Demand Instances \u2013 short workload, predictable pricing, pay by second Reserved (1 & 3 years) Reserved Instances \u2013 long workloads Convertible Reserved Instances \u2013 long workloads with flexible instances Savings Plans (1 & 3 years) \u2013commitment to an amount of usage, long workload Spot Instances \u2013 short workloads, cheap, can lose instances (less reliable) Dedicated Hosts \u2013 book an entire physical server, control instance placement Dedicated Instances \u2013 no other customers will share your hardware Capacity Reservations \u2013 reserve capacity in a specific AZ for any duration EC2 On Demand Pay for what you use: Linux or Windows - billing per second, after the first minute All other operating systems - billing per hour Has the highest cost but no upfront payment No long-term commitment Recommended for short-term and un-interrupted workloads, where you can't predict how the application will behave EC2 Reserved Instances Up to 72% discount compared to On-demand You reserve a specific instance attributes (Instance Type, Region, Tenancy, OS) Reservation Period \u2013 1 year (+discount) or 3 years (+++discount) Payment Options \u2013 No Upfront (+), Partial Upfront (++), All Upfront (+++) Reserved Instance\u2019s Scope \u2013 Regional or Zonal (reserve capacity in an AZ) Recommended for steady-state usage applications (think database) You can buy and sell in the Reserved Instance Marketplace Convertible Reserved Instance Can change the EC2 instance type, instance family, OS, scope and tenancy Up to 66% discount EC2 Savings Plans Get a discount based on long-term usage (up to 72% - same as RIs) Commit to a certain type of usage ($10/hour for 1 or 3 years) Usage beyond EC2 Savings Plans is billed at the On-Demand price Locked to a specific instance family & AWS region (e.g., M5 in us-east-1) Flexible across: Instance Size (e.g., m5.xlarge, m5.2xlarge) OS (e.g., Linux, Windows) Tenancy (Host, Dedicated, Default) EC2 Spot Instances Can get a discount of up to 90% compared to On-demand Instances that you can \u201close\u201d at any point of time if your max price is less than the current spot price The MOST cost-efficient instances in AWS Useful for workloads that are resilient to failure: Batch jobs Data analysis Image processing Any distributed workloads Workloads with a flexible start and end time Not suitable for critical jobs or databases EC2 Dedicated Hosts A physical server with EC2 instance capacity fully dedicated to your use Allows you address compliance requirements and use your existing server- bound software licenses (per-socket, per-core, pe\u2014VM software licenses) Purchasing Options: On-demand \u2013 pay per second for active Dedicated Host Reserved - 1 or 3 years (No Upfront, Partial Upfront, All Upfront) The most expensive option Useful for software that have complicated licensing model (BYOL \u2013 Bring Your Own License) Or for companies that have strong regulatory or compliance needs EC2 Dedicated Instances Instances run on hardware that\u2019s dedicated to you May share hardware with other instances in same account No control over instance placement (can move hardware after Stop / Start) EC2 Spot Instance Requests Can get a discount of up to 90% compared to On-demand Define max spot price and get the instance while current spot price < max The hourly spot price varies based on offer and capacity If the current spot price > your max price you can choose to stop or terminate your instance with a 2 minutes grace period. Other strategy: Spot Block \u201cblock\u201d spot instance during a specified time frame (1 to 6 hours) without interruptions In rare situations, the instance may be reclaimed Used for batch jobs, data analysis, or workloads that are resilient to failures. Not great for critical jobs or databases Terminating Spot Instance: You can only cancel Spot Instance requests that are open, active, or disabled. Cancelling a Spot Request does not terminate instances You must first cancel a Spot Request, and then terminate the associated Spot Instances Spot Fleets Spot Fleets = set of Spot Instances + (optional) On-Demand Instances The Spot Fleet will try to meet the target capacity with price constraints Define possible launch pools: instance type (m5.large), OS, Availability Zone Can have multiple launch pools, so that the fleet can choose Spot Fleet stops launching instances when reaching capacity or max cost Strategies to allocate Spot Instances: lowestPrice: from the pool with the lowest price (cost optimization, short workload) diversified: distributed across all pools (great for availability, long workloads) capacityOptimized: pool with the optimal capacity for the number of instances priceCapacityOptimized (recommended): pools with highest capacity available, then select the pool with the lowest price (best choice for most workloads) Spot Fleets allow us to automatically request Spot Instances with the lowest price Placement Groups Sometimes you want control over the EC2 Instance placement strategy That strategy can be defined using placement groups When you create a placement group, you specify one of the following strategies for the group: Cluster \u2014 clusters instances into a low-latency group in a single Availability Zone(In the same rack Pros: Great network (10 Gbps bandwidth between instances with Enhanced Networking enabled - recommended) Cons: If the rack fails, all instances fails at the same time Use case: Big Data job that needs to complete fast Application that needs extremely low latency and high network throughput) Spread \u2014 spreads instances across underlying hardware (max 7 instances per group per AZ) Pros: Can span across Availability Zones (AZ) Reduced risk is simultaneous failure EC2 Instances are on different physical hardware Cons: Limited to 7 instances per AZ per placement group Use case: Application that needs to maximize high availability Critical Applications where each instance must be isolated from failure from each other Partition\u2014spreads instances across many different partitions (which rely on different sets of racks) within an AZ. Scales to 100s of EC2 instances per group (Hadoop, Cassandra, Kafka) Up to 7 partitions per AZ Can span across multiple AZs in the same region Up to 100s of EC2 instances The instances in a partition do not share racks with the instances in the other partitions A partition failure can affect many EC2 but won\u2019t affect other partitions EC2 instances get access to the partition information as metadata Use cases: HDFS, HBase, Cassandra, Kafka Elastic Network Interfaces (ENI) https://aws.amazon.com/blogs/aws/new-elastic-network-interfaces-in-the-virtual-private-cloud/ Logical component in a VPC that represents a virtual network card The ENI can have the following attributes: Primary private IPv4, one or more secondary IPv4 One Elastic IP (IPv4) per private IPv4 One Public IPv4 One or more security groups A MAC address You can create ENI independently and attach them on the fly (move them) on EC2 instances for failover Bound to a specific availability zone (AZ) EC2 Hibernate Introducing EC2 Hibernate: The in-memory (RAM) state is preserved The instance boot is much faster! (the OS is not stopped / restarted) Under the hood: the RAM state is written to a file in the root EBS volume The root EBS volume must be encrypted Use cases: Long-running processing Saving the RAM state Services that take time to initialize Good to know: - Supported Instance Families \u2013 C3, C4, C5, I3, M3, M4, R3, R4, T2, T3, \u2026 - Instance RAM Size \u2013 must be less than 150 GB. - Instance Size \u2013 not supported for bare metal instances. - AMI \u2013 Amazon Linux 2, Linux AMI, Ubuntu, RHEL, CentOS & Windows\u2026 - Root Volume \u2013 must be EBS, encrypted, not instance store, and large - Available for On-Demand, Reserved and Spot Instances - An instance can NOT be hibernated more than 60 days","title":"01. EC2"},{"location":"01.%20EC2/#ec2-instances","text":"EC2 Instances Purchasing Options On-Demand Instances \u2013 short workload, predictable pricing, pay by second Reserved (1 & 3 years) Reserved Instances \u2013 long workloads Convertible Reserved Instances \u2013 long workloads with flexible instances Savings Plans (1 & 3 years) \u2013commitment to an amount of usage, long workload Spot Instances \u2013 short workloads, cheap, can lose instances (less reliable) Dedicated Hosts \u2013 book an entire physical server, control instance placement Dedicated Instances \u2013 no other customers will share your hardware Capacity Reservations \u2013 reserve capacity in a specific AZ for any duration EC2 On Demand Pay for what you use: Linux or Windows - billing per second, after the first minute All other operating systems - billing per hour Has the highest cost but no upfront payment No long-term commitment Recommended for short-term and un-interrupted workloads, where you can't predict how the application will behave EC2 Reserved Instances Up to 72% discount compared to On-demand You reserve a specific instance attributes (Instance Type, Region, Tenancy, OS) Reservation Period \u2013 1 year (+discount) or 3 years (+++discount) Payment Options \u2013 No Upfront (+), Partial Upfront (++), All Upfront (+++) Reserved Instance\u2019s Scope \u2013 Regional or Zonal (reserve capacity in an AZ) Recommended for steady-state usage applications (think database) You can buy and sell in the Reserved Instance Marketplace Convertible Reserved Instance Can change the EC2 instance type, instance family, OS, scope and tenancy Up to 66% discount EC2 Savings Plans Get a discount based on long-term usage (up to 72% - same as RIs) Commit to a certain type of usage ($10/hour for 1 or 3 years) Usage beyond EC2 Savings Plans is billed at the On-Demand price Locked to a specific instance family & AWS region (e.g., M5 in us-east-1) Flexible across: Instance Size (e.g., m5.xlarge, m5.2xlarge) OS (e.g., Linux, Windows) Tenancy (Host, Dedicated, Default) EC2 Spot Instances Can get a discount of up to 90% compared to On-demand Instances that you can \u201close\u201d at any point of time if your max price is less than the current spot price The MOST cost-efficient instances in AWS Useful for workloads that are resilient to failure: Batch jobs Data analysis Image processing Any distributed workloads Workloads with a flexible start and end time Not suitable for critical jobs or databases EC2 Dedicated Hosts A physical server with EC2 instance capacity fully dedicated to your use Allows you address compliance requirements and use your existing server- bound software licenses (per-socket, per-core, pe\u2014VM software licenses) Purchasing Options: On-demand \u2013 pay per second for active Dedicated Host Reserved - 1 or 3 years (No Upfront, Partial Upfront, All Upfront) The most expensive option Useful for software that have complicated licensing model (BYOL \u2013 Bring Your Own License) Or for companies that have strong regulatory or compliance needs EC2 Dedicated Instances Instances run on hardware that\u2019s dedicated to you May share hardware with other instances in same account No control over instance placement (can move hardware after Stop / Start) EC2 Spot Instance Requests Can get a discount of up to 90% compared to On-demand Define max spot price and get the instance while current spot price < max The hourly spot price varies based on offer and capacity If the current spot price > your max price you can choose to stop or terminate your instance with a 2 minutes grace period. Other strategy: Spot Block \u201cblock\u201d spot instance during a specified time frame (1 to 6 hours) without interruptions In rare situations, the instance may be reclaimed Used for batch jobs, data analysis, or workloads that are resilient to failures. Not great for critical jobs or databases Terminating Spot Instance: You can only cancel Spot Instance requests that are open, active, or disabled. Cancelling a Spot Request does not terminate instances You must first cancel a Spot Request, and then terminate the associated Spot Instances Spot Fleets Spot Fleets = set of Spot Instances + (optional) On-Demand Instances The Spot Fleet will try to meet the target capacity with price constraints Define possible launch pools: instance type (m5.large), OS, Availability Zone Can have multiple launch pools, so that the fleet can choose Spot Fleet stops launching instances when reaching capacity or max cost Strategies to allocate Spot Instances: lowestPrice: from the pool with the lowest price (cost optimization, short workload) diversified: distributed across all pools (great for availability, long workloads) capacityOptimized: pool with the optimal capacity for the number of instances priceCapacityOptimized (recommended): pools with highest capacity available, then select the pool with the lowest price (best choice for most workloads) Spot Fleets allow us to automatically request Spot Instances with the lowest price Placement Groups Sometimes you want control over the EC2 Instance placement strategy That strategy can be defined using placement groups When you create a placement group, you specify one of the following strategies for the group: Cluster \u2014 clusters instances into a low-latency group in a single Availability Zone(In the same rack Pros: Great network (10 Gbps bandwidth between instances with Enhanced Networking enabled - recommended) Cons: If the rack fails, all instances fails at the same time Use case: Big Data job that needs to complete fast Application that needs extremely low latency and high network throughput) Spread \u2014 spreads instances across underlying hardware (max 7 instances per group per AZ) Pros: Can span across Availability Zones (AZ) Reduced risk is simultaneous failure EC2 Instances are on different physical hardware Cons: Limited to 7 instances per AZ per placement group Use case: Application that needs to maximize high availability Critical Applications where each instance must be isolated from failure from each other Partition\u2014spreads instances across many different partitions (which rely on different sets of racks) within an AZ. Scales to 100s of EC2 instances per group (Hadoop, Cassandra, Kafka) Up to 7 partitions per AZ Can span across multiple AZs in the same region Up to 100s of EC2 instances The instances in a partition do not share racks with the instances in the other partitions A partition failure can affect many EC2 but won\u2019t affect other partitions EC2 instances get access to the partition information as metadata Use cases: HDFS, HBase, Cassandra, Kafka Elastic Network Interfaces (ENI) https://aws.amazon.com/blogs/aws/new-elastic-network-interfaces-in-the-virtual-private-cloud/ Logical component in a VPC that represents a virtual network card The ENI can have the following attributes: Primary private IPv4, one or more secondary IPv4 One Elastic IP (IPv4) per private IPv4 One Public IPv4 One or more security groups A MAC address You can create ENI independently and attach them on the fly (move them) on EC2 instances for failover Bound to a specific availability zone (AZ) EC2 Hibernate Introducing EC2 Hibernate: The in-memory (RAM) state is preserved The instance boot is much faster! (the OS is not stopped / restarted) Under the hood: the RAM state is written to a file in the root EBS volume The root EBS volume must be encrypted Use cases: Long-running processing Saving the RAM state Services that take time to initialize Good to know: - Supported Instance Families \u2013 C3, C4, C5, I3, M3, M4, R3, R4, T2, T3, \u2026 - Instance RAM Size \u2013 must be less than 150 GB. - Instance Size \u2013 not supported for bare metal instances. - AMI \u2013 Amazon Linux 2, Linux AMI, Ubuntu, RHEL, CentOS & Windows\u2026 - Root Volume \u2013 must be EBS, encrypted, not instance store, and large - Available for On-Demand, Reserved and Spot Instances - An instance can NOT be hibernated more than 60 days","title":"EC2 Instances"},{"location":"02.%20EC2%20Storage/","text":"EBS Volume An EBS (Elastic Block Store) Volume is a network drive you can attach to your instances while they run. It allows your instances to persist data, even after their termination They can only be mounted to one instance at a time (at the CCP level) They are bound to a specific availability zone Analogy: Think of them as a \u201cnetwork USB stick\u201d Free tier: 30 GB of free EBS storage of type General Purpose (SSD) or Magnetic per month It\u2019s a network drive (i.e. not a physical drive) It uses the network to communicate the instance, which means there might be a bit of latency It can be detached from an EC2 instance and attached to another one quickly It\u2019s locked to an Availability Zone (AZ) An EBS Volume in us-east-1a cannot be attached to us-east-1b To move a volume across, you first need to snapshot it Have a provisioned capacity (size in GBs, and IOPS) You get billed for all the provisioned capacity You can increase the capacity of the drive over time EBS \u2013 Delete on Termination attribute: Controls the EBS behaviour when an EC2 instance terminates By default, the root EBS volume is deleted (attribute enabled) By default, any other attached EBS volume is not deleted (attribute disabled) This can be controlled by the AWS console / AWS CLI Use case: preserve root volume when instance is terminated EBS Snapshots Make a backup (snapshot) of your EBS volume at a point in time Not necessary to detach volume to do snapshot, but recommended Can copy snapshots across AZ or Region EBS Snapshots Features EBS Snapshot Archive Move a Snapshot to an \u201darchive tier\u201d that is 75% cheaper Takes within 24 to 72 hours for restoring the archive Recycle Bin for EBS Snapshots Setup rules to retain deleted snapshots so you can recover them after an accidental deletion Specify retention (from 1 day to 1 year) Fast Snapshot Restore (FSR) Force full initialization of snapshot to have no latency on the first use ($$$) AMI Overview AMI = Amazon Machine Image AMI are a customization of an EC2 instance You add your own software, configuration, operating system, monitoring\u2026 Faster boot / configuration time because all your software is pre-packaged AMI are built for a specific region (and can be copied across regions) You can launch EC2 instances from: A Public AMI: AWS provided Your own AMI: you make and maintain them yourself An AWS Marketplace AMI: an AMI someone else made AMI Process (from an EC2 instance) - Start an EC2 instance and customize it - Stop the instance (for data integrity) - Build an AMI \u2013 this will also create EBS snapshots - Launch instances from other AMIs EC2 Instance Store EBS volumes are network drives with good but \u201climited\u201d performance If you need a high-performance hardware disk, use EC2 Instance Store Better I/O performance EC2 Instance Store lose their storage if they\u2019re stopped (ephemeral) Good for buffer / cache / scratch data / temporary content Risk of data loss if hardware fails Backups and Replication are your responsibility Local EC2 Instance Store EBS Volume Types EBS Volumes come in 6 types gp2 / gp3 (SSD): General purpose SSD volume that balances price and performance for a wide variety of workloads io1 / io2 Block Express (SSD): Highest-performance SSD volume for mission-critical low-latency or high-throughput workloads st1 (HDD): Low cost HDD volume designed for frequently accessed, throughput- intensive workloads sc1 (HDD): Lowest cost HDD volume designed for less frequently accessed workloads EBS Volumes are characterized in Size | Throughput | IOPS (I/O Ops Per Sec) When in doubt always consult the AWS documentation \u2013 it\u2019s good! Only gp2/gp3 and io1/io2 Block Express can be used as boot volumes EBS Volume Types Use cases General Purpose SSD Cost effective storage, low-latency System boot volumes, Virtual desktops, Development and test environments 1 GiB - 16 TiB gp3: Baseline of 3,000 IOPS and throughput of 125 MiB/s Can increase IOPS up to 16,000 and throughput up to 1000 MiB/s independently gp2: Small gp2 volumes can burst IOPS to 3,000 Size of the volume and IOPS are linked, max IOPS is 16,000 3 IOPS per GB, means at 5,334 GB we are at the max IOPS EBS Volume Types Use cases Provisioned IOPS (PIOPS) SSD Critical business applications with sustained IOPS performance Or applications that need more than 16,000 IOPS Great for databases workloads (sensitive to storage perf and consistency) io1 (4 GiB - 16 TiB): Max PIOPS: 64,000 for Nitro EC2 instances & 32,000 for other Can increase PIOPS independently from storage size io2 Block Express (4 GiB \u2013 64 TiB): Sub-millisecond latency Max PIOPS: 256,000 with an IOPS:GiB ratio of 1,000:1 Supports EBS Multi-attach EBS Volume Types Use cases Hard Disk Drives (HDD) Cannot be a boot volume 125 GiB to 16 TiB Throughput Optimized HDD (st1) Big Data, Data Warehouses, Log Processing Max throughput 500 MiB/s \u2013 max IOPS 500 Cold HDD (sc1): For data that is infrequently accessed Scenarios where lowest cost is important Max throughput 250 MiB/s \u2013 max IOPS 250 Don't need to get all details but some top level differences, like which one to use to get the most IOPS at the lowest cost for a specific use case EBS Multi-Attach \u2013 io1/io2 family Attach the same EBS volume to multiple EC2 instances in the same AZ Each instance has full read & write permissions to the high-performance volume Use case: Achieve higher application availability in clustered Linux applications (ex: Teradata) Applications must manage concurrent write operations Up to 16 EC2 Instances at a time Must use a file system that\u2019s cluster-aware (not XFS, EXT4, etc\u2026) EBS Encryption When you create an encrypted EBS volume, you get the following: Data at rest is encrypted inside the volume All the data in flight moving between the instance and the volume is encrypted All snapshots are encrypted All volumes created from the snapshot Encryption and decryption are handled transparently (you have nothing to do) Encryption has a minimal impact on latency EBS Encryption leverages keys from KMS (AES-256) Copying an unencrypted snapshot allows encryption Snapshots of encrypted volumes are encrypted Encryption: encrypt an unencrypted EBS volume Create an EBS snapshot of the volume Encrypt the EBS snapshot ( using copy ) Create new ebs volume from the snapshot ( the volume will also be encrypted ) Now you can attach the encrypted volume to the original instance Amazon EFS \u2013 Elastic File System Managed NFS (network file system) that can be mounted on many EC2 EFS works with EC2 instances in multi-AZ Highly available, scalable, expensive (3x gp2), pay per use Amazon EFS \u2013 Elastic File System Use cases: content management, web serving, data sharing, Wordpress Uses NFSv4.1 protocol Uses security group to control access to EFS Compatible with Linux based AMI (not Windows) Encryption at rest using KMS POSIX file system (~Linux) that has a standard file API File system scales automatically, pay-per-use, no capacity planning! EFS \u2013 Performance & Storage Classes EFS Scale 1000s of concurrent NFS clients, 10 GB+ /s throughput Grow to Petabyte-scale network file system, automatically Performance Mode (set at EFS creation time) General Purpose (default) \u2013 latency-sensitive use cases (web server, CMS, etc\u2026) Max I/O \u2013 higher latency, throughput, highly parallel (big data, media processing) Throughput Mode Bursting \u2013 1 TB = 50MiB/s + burst of up to 100MiB/s Provisioned \u2013 set your throughput regardless of storage size, ex: 1 GiB/s for 1 TB storage Elastic \u2013 automatically scales throughput up or down based on your workloads Up to 3GiB/s for reads and 1GiB/s for writes Used for unpredictable workloads EFS \u2013 Storage Classes Storage Tiers (lifecycle management feature \u2013 move file after N days) Standard: for frequently accessed files Infrequent access (EFS-IA): cost to retrieve files, lower price to store. Enable EFS-IA with a LifecyclePolicy Availability and durability Standard: Multi-AZ, great for prod One Zone: One AZ, great for dev, backup enabled by default, compatible with IA (EFS One Zone-IA) Over 90% in cost savings EBS vs EFS \u2013 Elastic Block Storage EBS volumes\u2026 one instance (except multi-attach io1/io2) are locked at the Availability Zone (AZ) level gp2: IO increases if the disk size increases gp3 & io1: can increase IO independently To migrate an EBS volume across AZ Take a snapshot Restore the snapshot to another AZ EBS backups use IO and you shouldn\u2019t run them while your application is handling a lot of traffic Root EBS Volumes of instances get terminated by default if the EC2 instanc EBS vs EFS \u2013 Elastic File System Mounting 100s of instances across AZ EFS share website files (WordPress) Only for Linux Instances (POSIX) EFS has a higher price point than EBS Can leverage EFS-IA for cost savings Remember: EFS vs EBS vs Instance Store","title":"02. EC2 Storage"},{"location":"03.%20ELB%20and%20ASG/","text":"ELASTIC LOAD BALANCER Why use a load balancer? Spread load across multiple downstream instances Expose a single point of access (DNS) to your application Seamlessly handle failures of downstream instances Do regular health checks to your instances Provide SSL termination (HTTPS) for your websites Enforce stickiness with cookies High availability across zones Separate public traffic from private traffic Why use an Elastic Load Balancer? An Elastic Load Balancer is a managed load balancer AWS guarantees that it will be working AWS takes care of upgrades, maintenance, high availability AWS provides only a few configuration knobs It costs less to setup your own load balancer but it will be a lot more effort on your end It is integrated with many AWS offerings / services EC2, EC2 Auto Scaling Groups, Amazon ECS AWS Certificate Manager (ACM), CloudWatch Route 53, AWS WAF, AWS Global Accelerator Health Checks Health Checks are crucial for Load Balancers They enable the load balancer to know if instances it forwards traffic to are available to reply to requests The health check is done on a port and a route (/health is common) If the response is not 200 (OK), then the instance is unhealthy Types of load balancer on AWS AWS has 4 kinds of managed Load Balancers Classic Load Balancer (v1 - old generation) \u2013 2009 \u2013 CLB HTTP, HTTPS, TCP, SSL (secure TCP) Supports TCP (Layer 4), HTTP & HTTPS (Layer 7) Health checks are TCP or HTTP based Fixed hostname XXX.region.elb.amazonaws.com Application Load Balancer (v2 - new generation) \u2013 2016 \u2013 ALB HTTP, HTTPS, WebSocket Load balancing to multiple HTTP applications across machines (target groups) Load balancing to multiple applications on the same machine (ex: containers) Support for HTTP/2 and WebSocket Support redirects (from HTTP to HTTPS for example) Routing tables to different target groups: Routing based on path in URL (example.com/users & example.com/posts) Routing based on hostname in URL (one.example.com & other.example.com) Routing based on Query String, Headers (example.com/users?id=123&order=false) ALB are a great fit for micro services & container-based application (example: Docker & Amazon ECS) Has a port mapping feature to redirect to a dynamic port in ECS In comparison, we\u2019d need multiple Classic Load Balancer per application Target Groups EC2 instances (can be managed by an Auto Scaling Group) \u2013 HTTP ECS tasks (managed by ECS itself) \u2013 HTTP Lambda functions \u2013 HTTP request is translated into a JSON event IP Addresses \u2013 must be private IPs ALB can route to multiple target groups Health checks are at the target group leve Network Load Balancer (v2 - new generation) \u2013 2017 \u2013 NLB TCP, TLS (secure TCP), UDP Network load balancers (Layer 4) allow to: Forward TCP & UDP traffic to your instances Handle millions of request per seconds Less latency ~100 ms (vs 400 ms for ALB) NLB has one static IP per AZ, and supports assigning Elastic IP (helpful for whitelisting specific IP) NLB are used for extreme performance, TCP or UDP traffic Not included in the AWS free tier Network Load Balancer \u2013Target Groups EC2 instances IP Addresses \u2013 must be private IPs Application Load Balancer (thanks to NLB you get fixed Ips and thanks tou ALB you get all rules and features) Health Checks support the TCP, HTTP and HTTPS Protocols Gateway Load Balancer \u2013 2020 \u2013 GWLB Operates at layer 3 (Network layer) \u2013 IP Protocol https://synechron.udemy.com/course/aws-certified-solutions-architect-associate-saa-c03/learn/lecture/28874688#overview Deploy, scale, and manage a fleet of 3rd party network virtual appliances in AWS Example: Firewalls, Intrusion Detection and Prevention Systems, Deep Packet Inspection Systems, payload manipulation, \u2026 Operates at Layer 3 (Network Layer) \u2013 IP Packets Combines the following functions: Transparent Network Gateway \u2013 single entry/exit for all traffic Load Balancer \u2013 distributes traffic to your virtual appliances Uses the GENEVE protocol on port 6081 Gateway Load Balancer \u2013Target Groups EC2 instances IP Addresses \u2013 must be private IPs Overall, it is recommended to use the newer generation load balancers as they provide more features Some load balancers can be setup as internal (private) or external (public) ELBs Sticky Sessions (Session Affinity) It is possible to implement stickiness so that the same client is always redirected to the same instance behind a load balancer This works for Classic Load Balancer, Application Load Balancer, and Network Load Balancer For both CLB & ALB, the \u201ccookie\u201d used for stickiness has an expiration date you control Use case: make sure the user doesn\u2019t lose his session data Enabling stickiness may bring imbalance to the load over the backend EC2 instances Sticky Sessions \u2013 Cookie Names This is configured inside target group(attributes) Application-based Cookies Custom cookie Generated by the target Can include any custom attributes required by the application Cookie name must be specified individually for each target group Don\u2019t use AWSALB, AWSALBAPP, or AWSALBTG (reserved for use by the ELB) Application cookie Generated by the load balancer Cookie name is AWSALBAPP Duration-based Cookies Cookie generated by the load balancer Cookie name is AWSALB for ALB, AWSELB for CLB Cross-Zone Load Balancing With Cross Zone Load Balancing: each load balancer instance distributes evenly across all registered instances in all AZ Without Cross Zone Load Balancing: Requests are distributed in the instances of the node of the Elastic Load Balancer It is configured on the ELB itself, attributes section Application Load Balancer Enabled by default (can be disabled at the Target Group level) No charges for inter AZ data Network Load Balancer & Gateway Load Balancer Disabled by default You pay charges ($) for inter AZ data if enabled Classic Load Balancer Disabled by default No charges for inter AZ data if enabled Load Balancer - SSL Certificates The load balancer uses an X.509 certificate (SSL/TLS server certificate) You can manage certificates using ACM (AWS Certificate Manager) You can create upload your own certificates alternatively HTTPS listener: You must specify a default certificate You can add an optional list of certs to support multiple domains Clients can use SNI (Server Name Indication) to specify the hostname they reach Ability to specify a security policy to support older versions of SSL / TLS (legacy clients) SSL \u2013 Server Name Indication (SNI) SNI solves the problem of loading multiple SSL certificates onto one web server (to serve multiple websites) It\u2019s a \u201cnewer\u201d protocol, and requires the client to indicate the hostname of the target server in the initial SSL handshake The server will then find the correct certificate, or return the default one Note: - Only works for ALB & NLB (newer generation), CloudFront - Does not work for CLB (older gen) Elastic Load Balancers \u2013 SSL Certificates Classic Load Balancer (v1) Support only one SSL certificate Must use multiple CLB for multiple hostname with multiple SSL certificates Application Load Balancer (v2) Supports multiple listeners with multiple SSL certificates Uses Server Name Indication (SNI) to make it work Network Load Balancer (v2) Supports multiple listeners with multiple SSL certificates Uses Server Name Indication (SNI) to make it work Connection Draining Feature naming Connection Draining \u2013 for CLB Deregistration Delay \u2013 for ALB & NLB Time to complete \u201cin-flight requests\u201d while the instance is de-registering or unhealthy Stops sending new requests to the EC2 instance which is de-registering Between 1 to 3600 seconds (default: 300 seconds) Can be disabled (set value to 0) Set to a low value if your requests are short AUTO SCALING GROUP What\u2019s an Auto Scaling Group? In real-life, the load on your websites and application can change In the cloud, you can create and get rid of servers very quickly The goal of an Auto Scaling Group (ASG) is to: Scale out (add EC2 instances) to match an increased load Scale in (remove EC2 instances) to match a decreased load Ensure we have a minimum and a maximum number of EC2 instances running Automatically register new instances to a load balancer Re-create an EC2 instance in case a previous one is terminated (ex: if unhealthy) ASG are free (you only pay for the underlying EC2 instances) Auto Scaling Group Attributes A Launch Template (older \u201cLaunch Configurations\u201d are deprecated) AMI + Instance Type EC2 User Data EBS Volumes Security Groups SSH Key Pair IAM Roles for your EC2 Instances Network + Subnets Information Load Balancer Information Min Size / Max Size / Initial Capacity Scaling Policies Auto Scaling - CloudWatch Alarms & Scaling It is possible to scale an ASG based on CloudWatch alarms An alarm monitors a metric (such as Average CPU, or a custom metric) Metrics such as Average CPU are computed for the overall ASG instances Based on the alarm: We can create scale-out policies (increase the number of instances) We can create scale-in policies (decrease the number of instances) Auto Scaling Groups \u2013 Scaling Policies Dynamic Scaling Target Tracking Scaling Simple to set-up Example: I want the average ASG CPU to stay at around 40% Simple / Step Scaling When a CloudWatch alarm is triggered (example CPU > 70%), then add 2 units When a CloudWatch alarm is triggered (example CPU < 30%), then remove 1 Scheduled Scaling Anticipate a scaling based on known usage patterns Example: increase the min capacity to 10 at 5 pm on Fridays Predictive scaling: continuously forecast load and schedule scaling ahead Good metrics to scale on CPUUtilization: Average CPU utilization across your instances RequestCountPerTarget: to make sure the number of requests per EC2 instances is stable Average Network In / Out (if you\u2019re application is network bound) Any custom metric (that you push using CloudWatch) Auto Scaling Groups - Scaling Cooldowns After a scaling activity happens, you are in the cooldown period (default 300 seconds) During the cooldown period, the ASG will not launch or terminate additional instances (to allow for metrics to stabilize) Advice: Use a ready-to-use AMI to reduce configuration time in order to be serving request fasters and reduce the cooldown period","title":"03. ELB and ASG"},{"location":"03.%20ELB%20and%20ASG/#elastic-load-balancer","text":"Why use a load balancer? Spread load across multiple downstream instances Expose a single point of access (DNS) to your application Seamlessly handle failures of downstream instances Do regular health checks to your instances Provide SSL termination (HTTPS) for your websites Enforce stickiness with cookies High availability across zones Separate public traffic from private traffic Why use an Elastic Load Balancer? An Elastic Load Balancer is a managed load balancer AWS guarantees that it will be working AWS takes care of upgrades, maintenance, high availability AWS provides only a few configuration knobs It costs less to setup your own load balancer but it will be a lot more effort on your end It is integrated with many AWS offerings / services EC2, EC2 Auto Scaling Groups, Amazon ECS AWS Certificate Manager (ACM), CloudWatch Route 53, AWS WAF, AWS Global Accelerator Health Checks Health Checks are crucial for Load Balancers They enable the load balancer to know if instances it forwards traffic to are available to reply to requests The health check is done on a port and a route (/health is common) If the response is not 200 (OK), then the instance is unhealthy Types of load balancer on AWS AWS has 4 kinds of managed Load Balancers Classic Load Balancer (v1 - old generation) \u2013 2009 \u2013 CLB HTTP, HTTPS, TCP, SSL (secure TCP) Supports TCP (Layer 4), HTTP & HTTPS (Layer 7) Health checks are TCP or HTTP based Fixed hostname XXX.region.elb.amazonaws.com Application Load Balancer (v2 - new generation) \u2013 2016 \u2013 ALB HTTP, HTTPS, WebSocket Load balancing to multiple HTTP applications across machines (target groups) Load balancing to multiple applications on the same machine (ex: containers) Support for HTTP/2 and WebSocket Support redirects (from HTTP to HTTPS for example) Routing tables to different target groups: Routing based on path in URL (example.com/users & example.com/posts) Routing based on hostname in URL (one.example.com & other.example.com) Routing based on Query String, Headers (example.com/users?id=123&order=false) ALB are a great fit for micro services & container-based application (example: Docker & Amazon ECS) Has a port mapping feature to redirect to a dynamic port in ECS In comparison, we\u2019d need multiple Classic Load Balancer per application Target Groups EC2 instances (can be managed by an Auto Scaling Group) \u2013 HTTP ECS tasks (managed by ECS itself) \u2013 HTTP Lambda functions \u2013 HTTP request is translated into a JSON event IP Addresses \u2013 must be private IPs ALB can route to multiple target groups Health checks are at the target group leve Network Load Balancer (v2 - new generation) \u2013 2017 \u2013 NLB TCP, TLS (secure TCP), UDP Network load balancers (Layer 4) allow to: Forward TCP & UDP traffic to your instances Handle millions of request per seconds Less latency ~100 ms (vs 400 ms for ALB) NLB has one static IP per AZ, and supports assigning Elastic IP (helpful for whitelisting specific IP) NLB are used for extreme performance, TCP or UDP traffic Not included in the AWS free tier Network Load Balancer \u2013Target Groups EC2 instances IP Addresses \u2013 must be private IPs Application Load Balancer (thanks to NLB you get fixed Ips and thanks tou ALB you get all rules and features) Health Checks support the TCP, HTTP and HTTPS Protocols Gateway Load Balancer \u2013 2020 \u2013 GWLB Operates at layer 3 (Network layer) \u2013 IP Protocol https://synechron.udemy.com/course/aws-certified-solutions-architect-associate-saa-c03/learn/lecture/28874688#overview Deploy, scale, and manage a fleet of 3rd party network virtual appliances in AWS Example: Firewalls, Intrusion Detection and Prevention Systems, Deep Packet Inspection Systems, payload manipulation, \u2026 Operates at Layer 3 (Network Layer) \u2013 IP Packets Combines the following functions: Transparent Network Gateway \u2013 single entry/exit for all traffic Load Balancer \u2013 distributes traffic to your virtual appliances Uses the GENEVE protocol on port 6081 Gateway Load Balancer \u2013Target Groups EC2 instances IP Addresses \u2013 must be private IPs Overall, it is recommended to use the newer generation load balancers as they provide more features Some load balancers can be setup as internal (private) or external (public) ELBs Sticky Sessions (Session Affinity) It is possible to implement stickiness so that the same client is always redirected to the same instance behind a load balancer This works for Classic Load Balancer, Application Load Balancer, and Network Load Balancer For both CLB & ALB, the \u201ccookie\u201d used for stickiness has an expiration date you control Use case: make sure the user doesn\u2019t lose his session data Enabling stickiness may bring imbalance to the load over the backend EC2 instances Sticky Sessions \u2013 Cookie Names This is configured inside target group(attributes) Application-based Cookies Custom cookie Generated by the target Can include any custom attributes required by the application Cookie name must be specified individually for each target group Don\u2019t use AWSALB, AWSALBAPP, or AWSALBTG (reserved for use by the ELB) Application cookie Generated by the load balancer Cookie name is AWSALBAPP Duration-based Cookies Cookie generated by the load balancer Cookie name is AWSALB for ALB, AWSELB for CLB Cross-Zone Load Balancing With Cross Zone Load Balancing: each load balancer instance distributes evenly across all registered instances in all AZ Without Cross Zone Load Balancing: Requests are distributed in the instances of the node of the Elastic Load Balancer It is configured on the ELB itself, attributes section Application Load Balancer Enabled by default (can be disabled at the Target Group level) No charges for inter AZ data Network Load Balancer & Gateway Load Balancer Disabled by default You pay charges ($) for inter AZ data if enabled Classic Load Balancer Disabled by default No charges for inter AZ data if enabled Load Balancer - SSL Certificates The load balancer uses an X.509 certificate (SSL/TLS server certificate) You can manage certificates using ACM (AWS Certificate Manager) You can create upload your own certificates alternatively HTTPS listener: You must specify a default certificate You can add an optional list of certs to support multiple domains Clients can use SNI (Server Name Indication) to specify the hostname they reach Ability to specify a security policy to support older versions of SSL / TLS (legacy clients) SSL \u2013 Server Name Indication (SNI) SNI solves the problem of loading multiple SSL certificates onto one web server (to serve multiple websites) It\u2019s a \u201cnewer\u201d protocol, and requires the client to indicate the hostname of the target server in the initial SSL handshake The server will then find the correct certificate, or return the default one Note: - Only works for ALB & NLB (newer generation), CloudFront - Does not work for CLB (older gen) Elastic Load Balancers \u2013 SSL Certificates Classic Load Balancer (v1) Support only one SSL certificate Must use multiple CLB for multiple hostname with multiple SSL certificates Application Load Balancer (v2) Supports multiple listeners with multiple SSL certificates Uses Server Name Indication (SNI) to make it work Network Load Balancer (v2) Supports multiple listeners with multiple SSL certificates Uses Server Name Indication (SNI) to make it work Connection Draining Feature naming Connection Draining \u2013 for CLB Deregistration Delay \u2013 for ALB & NLB Time to complete \u201cin-flight requests\u201d while the instance is de-registering or unhealthy Stops sending new requests to the EC2 instance which is de-registering Between 1 to 3600 seconds (default: 300 seconds) Can be disabled (set value to 0) Set to a low value if your requests are short","title":"ELASTIC LOAD BALANCER"},{"location":"03.%20ELB%20and%20ASG/#auto-scaling-group","text":"What\u2019s an Auto Scaling Group? In real-life, the load on your websites and application can change In the cloud, you can create and get rid of servers very quickly The goal of an Auto Scaling Group (ASG) is to: Scale out (add EC2 instances) to match an increased load Scale in (remove EC2 instances) to match a decreased load Ensure we have a minimum and a maximum number of EC2 instances running Automatically register new instances to a load balancer Re-create an EC2 instance in case a previous one is terminated (ex: if unhealthy) ASG are free (you only pay for the underlying EC2 instances) Auto Scaling Group Attributes A Launch Template (older \u201cLaunch Configurations\u201d are deprecated) AMI + Instance Type EC2 User Data EBS Volumes Security Groups SSH Key Pair IAM Roles for your EC2 Instances Network + Subnets Information Load Balancer Information Min Size / Max Size / Initial Capacity Scaling Policies Auto Scaling - CloudWatch Alarms & Scaling It is possible to scale an ASG based on CloudWatch alarms An alarm monitors a metric (such as Average CPU, or a custom metric) Metrics such as Average CPU are computed for the overall ASG instances Based on the alarm: We can create scale-out policies (increase the number of instances) We can create scale-in policies (decrease the number of instances) Auto Scaling Groups \u2013 Scaling Policies Dynamic Scaling Target Tracking Scaling Simple to set-up Example: I want the average ASG CPU to stay at around 40% Simple / Step Scaling When a CloudWatch alarm is triggered (example CPU > 70%), then add 2 units When a CloudWatch alarm is triggered (example CPU < 30%), then remove 1 Scheduled Scaling Anticipate a scaling based on known usage patterns Example: increase the min capacity to 10 at 5 pm on Fridays Predictive scaling: continuously forecast load and schedule scaling ahead Good metrics to scale on CPUUtilization: Average CPU utilization across your instances RequestCountPerTarget: to make sure the number of requests per EC2 instances is stable Average Network In / Out (if you\u2019re application is network bound) Any custom metric (that you push using CloudWatch) Auto Scaling Groups - Scaling Cooldowns After a scaling activity happens, you are in the cooldown period (default 300 seconds) During the cooldown period, the ASG will not launch or terminate additional instances (to allow for metrics to stabilize) Advice: Use a ready-to-use AMI to reduce configuration time in order to be serving request fasters and reduce the cooldown period","title":"AUTO SCALING GROUP"},{"location":"04.%20RDS-Aurora-ElastiCache/","text":"Amazon RDS Overview Tool for connecting to DB sqlectron -> https://github.com/sqlectron/sqlectron-gui/releases/tag/v1.38 .0 RDS stands for Relational Database Service It\u2019s a managed DB service for DB use SQL as a query language. It allows you to create databases in the cloud that are managed by AWS Postgres MySQL MariaDB Oracle Microsoft SQL Server Aurora (AWS Proprietary database) Advantage over using RDS versus deploying DB on EC2 RDS is a managed service: Automated provisioning, OS patching Continuous backups and restore to specific timestamp (Point in Time Restore)! Monitoring dashboards Read replicas for improved read performance Multi AZ setup for DR (Disaster Recovery) Maintenance windows for upgrades Scaling capability (vertical and horizontal) Storage backed by EBS (gp2 or io1) BUT you can\u2019t SSH into your instances RDS \u2013 Storage Auto Scaling Helps you increase storage on your RDS DB instance dynamically When RDS detects you are running out of free database storage, it scales automatically Avoid manually scaling your database storage You have to set Maximum Storage Threshold (maximum limit for DB storage) Automatically modify storage if: Free storage is less than 10% of allocated storage Low-storage lasts at least 5 minutes 6 hours have passed since last modification Useful for applications with unpredictable workloads Supports all RDS database engines (MariaDB, MySQL, PostgreSQL, SQL Server, Oracle) RDS Read Replicas for read scalability Up to 15 Read Replicas Within AZ, Cross AZ or Cross Region Replication is ASYNC, so reads are eventually consistent Replicas can be promoted to their own DB Applications must update the connection string to leverage read replicas RDS Read Replicas \u2013 Use Cases You have a production database that is taking on normal load You want to run a reporting application to run some analytics You create a Read Replica to run the new workload there The production application is unaffected Read replicas are used for SELECT (=read) only kind of statements (not INSERT, UPDATE, DELETE) RDS Read Replicas \u2013 Network Cost In AWS there\u2019s a network cost when data goes from one AZ to another For RDS Read Replicas within the same region, you don\u2019t pay that fee RDS Multi AZ (Disaster Recovery) SYNC replication One DNS name \u2013 automatic app failover to standby Increase availability Failover in case of loss of AZ, loss of network, instance or storage failure No manual intervention in apps Not used for scaling Note:The Read Replicas be setup as Multi AZ for Disaster Recovery (DR) RDS \u2013 From Single-AZ to Multi-AZ Zero downtime operation (no need to stop the DB) Just click on \u201cmodify\u201d for the database The following happens internally: A snapshot is taken A new DB is restored from the snapshot in a new AZ Synchronization is established between the two databases RDS Custom Managed Oracle and Microsoft SQL Server Database with OS and database customization RDS: Automates setup, operation, and scaling of database in AWS Custom: access to the underlying database and OS so you can Configure settings Install patches Enable native features Access the underlying EC2 Instance using SSH or SSM Session Manager De-activate Automation Mode to perform your customization, better to take a DB snapshot before RDS vs. RDS Custom RDS: entire database and the OS to be managed by AWS RDS Custom: full admin access to the underlying OS and the database Amazon Aurora Aurora is a proprietary technology from AWS (not open sourced) Postgres and MySQL are both supported as Aurora DB (that means your drivers will work as if Aurora was a Postgres or MySQL database) Aurora is \u201cAWS cloud optimized\u201d and claims 5x performance improvement over MySQL on RDS, over 3x the performance of Postgres on RDS Aurora storage automatically grows in increments of 10GB, up to 128 TB. Aurora can have up to 15 replicas and the replication process is faster than MySQL (sub 10 ms replica lag) Failover in Aurora is instantaneous. It\u2019s HA (High Availability) native. Aurora costs more than RDS (20% more) \u2013 but is more efficient Aurora High Availability and Read Scaling 6 copies of your data across 3 AZ: 4 copies out of 6 needed for writes 3 copies out of 6 need for reads Self healing with peer-to-peer replication Storage is striped across 100s of volumes One Aurora Instance takes writes (master) Automated failover for master in less than 30 seconds Master + up to 15 Aurora Read Replicas serve reads Support for Cross Region Replication Aurora DB Cluster Features of Aurora Automatic fail-over Backup and Recovery Isolation and security Industry compliance Push-button scaling Automated Patching with Zero Downtime Advanced Monitoring Routine Maintenance Backtrack: restore data at any point of time without using backups Aurora Replicas - Auto Scaling Aurora \u2013 Custom Endpoints Define a subset of Aurora Instances as a Custom Endpoint Example: Run analytical queries on specific replicas The Reader Endpoint is generally not used after defining Custom Endpoints Aurora Serverless Automated database instantiation and auto - scaling based on actual usage Good for infrequent, intermittent or unpredictable workloads No capacity planning needed Pay per second, can be more cost-effective Global Aurora Aurora Cross Region Read Replicas: Useful for disaster recovery Simple to put in place Aurora Global Database (recommended): 1 Primary Region (read / write) Up to 5 secondary (read-only) regions, replication lag is less than 1 second Up to 16 Read Replicas per secondary region Helps for decreasing latency Promoting another region (for disaster recovery) has an RTO of < 1 minute Typical cross-region replication takes less than 1 second Aurora Machine Learning Enables you to add ML-based predictions to your applications via SQL Simple, optimized, and secure integration between Aurora and AWS ML services Supported services Amazon SageMaker (use with any ML model) Amazon Comprehend (for sentiment analysis) You don\u2019t need to have ML experience Use cases: fraud detection, ads targeting, sentiment analysis, product recommendations RDS Backups Automated backups: Daily full backup of the database (during the backup window) Transaction logs are backed-up by RDS every 5 minutes => ability to restore to any point in time (from oldest backup to 5 minutes ago) 1 to 35 days of retention, set 0 to disable automated backups Manual DB Snapshots Manually triggered by the user Retention of backup for as long as you want Trick: in a stopped RDS database, you will still pay for storage. If you plan on stopping it for a long time, you should snapshot & restore instead Aurora Backups Automated backups 1 to 35 days (cannot be disabled) point-in-time recovery in that timeframe Manual DB Snapshots Manually triggered by the user Retention of backup for as long as you want RDS & Aurora Restore options Restoring a RDS / Aurora backup or a snapshot creates a new database Restoring MySQL RDS database from S3 Create a backup of your on-premises database Store it on Amazon S3 (object storage) Restore the backup file onto a new RDS instance running MySQL Restoring MySQL Aurora cluster from S3 Create a backup of your on-premises database using Percona XtraBackup Store the backup file on Amazon S3 Restore the backup file onto a new Aurora cluster running MySQL Aurora Database Cloning Create a new Aurora DB Cluster from an existing one Faster than snapshot & restore Uses copy-on-write protocol Initially, the new DB cluster uses the same data volume as the original DB cluster (fast and efficient \u2013 no copying is needed) When updates are made to the new DB clusterdata, then additional storage is allocated and data is copied to be separated Very fast & cost-effective Useful to create a \u201cstaging\u201d database from a \u201cproduction\u201d database without impacting the production database","title":"04. RDS Aurora ElastiCache"},{"location":"05.%20Databases/","text":"Choosing the Right Database We have a lot of managed databases on AWS to choose from Questions to choose the right database based on your architecture: Read-heavy, write-heavy, or balanced workload? Throughput needs? Will it change, does it need to scale or fluctuate during the day? How much data to store and for how long? Will it grow? Average object size? How are they accessed? Data durability? Source of truth for the data ? Latency requirements? Concurrent users? Data model? How will you query the data? Joins? Structured? Semi-Structured? Strong schema? More flexibility? Reporting? Search? RDBMS / NoSQL? License costs? Switch to Cloud Native DB such as Aurora? Database Types RDBMS (= SQL / OLTP): RDS, Aurora \u2013 great for joins NoSQL database \u2013 no joins, no SQL : DynamoDB (~JSON), ElastiCache (key / value pairs), Neptune (graphs), DocumentDB (for MongoDB), Keyspaces (for Apache Cassandra) Object Store: S3 (for big objects) / Glacier (for backups / archives) Data Warehouse (= SQL Analytics / BI): Redshift (OLAP), Athena, EMR Search: OpenSearch (JSON) \u2013 free text, unstructured searches Graphs: Amazon Neptune \u2013 displays relationships between data Ledger: Amazon Quantum Ledger Database Time series: Amazon Timestream Note: some databases are being discussed in the Data & Analytics section","title":"05. Databases"},{"location":"10.%20AWS%20Organizations/","text":"","title":"10. AWS Organizations"}]}